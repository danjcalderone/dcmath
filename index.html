<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"></link>
    <meta http-equiv="X-UA-Compatible" content="ie=edge" ></link>

    <link
    rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/katex@0.13.2/dist/katex.min.css"
    integrity="sha384-Cqd8ihRLum0CCg8rz0hYKPoLZ3uw+gES2rXQXycqnL5pgVQIflxAUDS7ZSjITLb5"
    crossorigin="anonymous"></link>

  <!-- The loading of KaTeX is deferred to speed up page rendering -->
  <script
    defer
    src="https://cdn.jsdelivr.net/npm/katex@0.13.2/dist/katex.min.js"
    integrity="sha384-1Or6BdeNQb0ezrmtGeqQHFpppNd7a/gw29xeiSikBbsb44xu3uAo8c7FwbF5jhbd"
    crossorigin="anonymous"></script>
  <!-- To automatically render math in text elements, include the auto-render extension: -->

  <script
    defer
    src="https://cdn.jsdelivr.net/npm/katex@0.13.2/dist/contrib/auto-render.min.js"
    integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl"
    crossorigin="anonymous"
    onload="renderMathInElement(document.body)">
    </script>
<!--
    {delimiters: [
      {left: "$$", right: "$$", display: true},
      {left: "$", right: "$", display: false},
      {left: "\\begin{equation}", right: "\\end{equation}", display: true},
      {left: "\\begin{align}", right: "\\end{align}", display: true},
      {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
      {left: "\\begin{gather}", right: "\\end{gather}", display: true},
      {left: "\\(", right: "\\)", display: false},
      {left: "\\[", right: "\\]", display: true}
      ]
      }
      );">
 -->






  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjs/6.2.5/math.min.js"> </script>
  <script src="https://unpkg.com/pts@0.10.5/dist/pts.js"></script>

  <!-- <link rel="stylesheet" href="./styles.css" ></link> -->

  <style>

      html {
        font-family: "Montserrat", sans-serif;
      }
      body {
        padding: 0;
        margin: 0;
      }

      header {
        background: rgba(64, 106, 157, 1);
        padding: 16px 32px;
        min-height: 10px;
      }

      header h1 {
        color: #ececec;
        margin-top: 20px;
      }

      header p {
        color: #e2e1e1;
        margin-top: 24px;
      }

      nav ul {
        padding: 0;
        margin: 0;
        background: #6c98b6c7;
        list-style: none;
        overflow: hidden;
      }

      nav li {
        float: left;
        font-size: 20px;
      }

      nav li a {
        display: block;
        padding: 10px 32px;
        text-decoration: none;
        color: #ececec;
      }

      nav li a:hover {
        color: #583d72;
      }

      nav li a:active {
        color: #ececec;
      }

      .main1 {
        background-color: rgba(0, 0, 255, 0);
        height: 800px;
        width: 800px;
        position: absolute;
        z-index: 1;
      }

      .EQN {
        background-color: rgba(0, 0, 255, 0);
        color: black;
        position: absolute;
        font-size: 16px;
        top: 623px;
        left: 150px;
        z-index: -1;
      }


      .col2-container {
      display: grid;
      padding: 0px;
      grid-template-columns:  400px, 300px;
      }
      .blurbASDF {
        margin: 40px;
        padding: 30px;
        background-color: rgba(64, 106, 157, 0.1);
        font-size: 16px;
        z-index: 1;
      }
      .blurb2ASDF {
        margin: 40px;
        padding: 20px;
        height: auto;
        background-color: rgba(64, 106, 157, 0.1);
        font-size: 16px;
      }






      .col2 {
        display: flex;
        flex-flow: row wrap;
        justify-content: flex-start;
        align-items: flex-start;
      }


      .img1 {
        flex: 0 1 auto;
        padding: 0px;
        position: relative;
        width: 900px;
        height: 900px;
      }


      .blurb1 {
        flex: 0 1 auto;
          margin: 40px;
          padding: 40px;
          width: 300px;
          background-color: rgba(64, 106, 157, 0.1);
          font-size: 16px;
        }

      @media all and (max-width:1500px) {
        .col2 {justify-content: center}
        .blurb1 {width:900px; height: auto}
        .img1 {width:900px; height: 900px}
      }


      .CANVAS {
        background-color: rgb(0, 0, 255, 0);
        z-index: 1;
        top: 0;
        left: 0;
        width: 900px;
        height: 900px;
      }

      .eqn1 {
        background-color: rgba(0, 0, 255, 0);
        color: black;
        position: absolute;
        font-size: 16px;
        top: 0px;
        left: 0px;
        z-index: -1;
      }







  </style>

  </head>

  <body>

  <header> <h1> .dcmath - UNDER CONSTRUCTION </h1> </header>
  <nav>
    <ul>
      <li><a href="index.html">vectors</a></li>
      <!-- <li><a href="matrix.html">matrix</a></li> -->
      <li><a style="background-color: rgba(100,0,0,0.2);" href="https://danjcalderone.github.io/"> danjcalderone </a></li>
    </ul>
  </nav>

<div class=col2>
      <div class='img1' style="padding:0px" id="hypershape-container">
        <canvas id='hypershape' class='CANVAS'></canvas>
      </div>

      <div class="blurb1" style="padding:40px">

    <b> HYPERSHAPES </b>
    <p>
    We're used to drawing 2D projections of 3D shapes.
    This process can be extended to n-dimensions by defining a
    2D direction for each of the n coordinate axes.
    Modifying these 2D directions gives a different perspective
    on the nD shape.
    This example shows the result for various shapes
    including the unit simplex, unit cube, and unit sphere.
  </p>
    <p>
    "Depth" is the dimension(s) that are lost in the 2D projection.
    For an n-dimensional shape, depth is n-2 dimensional.  When drawing a 3D shape, depth is 1-dimensional,
    ie. there is a line of points (coming towards the camera) that are
    indistinguishable in the picture.
    When drawing a 4D shape, depth is 2-dimensional, ie. there is a plane of points
    (in the 4D space) that are indistinguishable.
    When drawing a 5D shape, depth is 3-dimensional...
  </p>

  <p>
    We can't fully "see" n-dimensions, but we can see 2D or 3D subspaces
    of the nD space.  The question
    then becomes can we trick our brains into stitching those 2D/3D
    subspaces back together into some "nD" image.
    The answer remains unclear.

  </p>

        </div>



  </div>



  <div class=col2>
    <div class='img1' style="padding:0px" id="vector-container">
            <canvas id='vector' class='CANVAS'></canvas>
            <div class="eqn1" id="y_vec" ></div>
            <div class="eqn1" id="A_vec" ></div>
            <div class="eqn1" id="x_vec" ></div>
            <div class="eqn1" id="Amaps_vec" ></div>
            <div class="eqn1" id="label2_vec" ></div>
            <div class="eqn1" id="warning2_vec" ></div>
            <div class="eqn1" id="usedigits_vec" ></div>
    </div>
    <div class="blurb1" style="padding:40px">

          <b> VECTORS & LINEAR COMBINATIONS </b>
          <p>

          The Cartesian representation of a mD vector is a string of \(m\) numbers that each
          represent a distance in a different direction/dimension.
          </p>
          <p>

          Addition between two vectors \(x\) and \(z\) is done element-wise (assuming they have the same length).

          $$
          x + z = \begin{bmatrix} x_1 \\ \vdots \\ x_m \end{bmatrix}
          + \begin{bmatrix} z_1 \\ \vdots \\ z_m \end{bmatrix} =
          \begin{bmatrix} x_1+z_1 \\ \vdots \\ x_m+z_m \end{bmatrix}
          $$

          Visually, this sum is a new vector given by placing the two vectors "tip-to-tail."
        </p>

        <p>
          "Scaling" \(x\) by a number \(\lambda\) involves multiplying each element in \(x\) by \(\lambda\).
          Visually this expands or shrinks (or flips) \(x\) without changing the direction.

          Scaling multiple vectors \(x^1,\dots,x^n\) and then adding them together is referred to as
          taking a "linear combination" of the vectors.  All the possible linear combinations of a set of vectors
          is called the "span" of those vectors.  If a set of vectors span a whole space that means
          any vector in that space can be written as a linear combination of those vectors.


        </p>

        </div>
      </div>



      <div class=col2>
        <div class='img1' style="padding:0px" id="matrix-container">
                <canvas id='matrix' class='CANVAS'></canvas>
                <div class="eqn1" id="y_mat" ></div>
                <div class="eqn1" id="A_mat" ></div>
                <div class="eqn1" id="x_mat" ></div>
                <div class="eqn1" id="Amaps_mat" ></div>
                <div class="eqn1" id="label2_mat" ></div>
                <div class="eqn1" id="warning2_mat" ></div>
                <div class="eqn1" id="usedigits_mat" ></div>
        </div>
        <div class="blurb1" style="padding:40px">

              <b> LINEAR TRANSFORMATIONS </b>
              <p>

              $$ y = Ax$$

              A matrix \( A \) can be used to represent a linear transformation from one
              vector space another. Any point \(x\) in the domain maps to \(y=Ax\) in the co-domain.
              Each standard basis vector gets mapped to
              the corresponding column of \(A\). For example,

              $$
              \begin{bmatrix}
              | & | & & | \\
              A_1  & A_2  & \cdots & A_n \\
              | & | & & | \\
              \end{bmatrix}
              \begin{bmatrix} 0 \\ 1 \\ \vdots \\ 0 \end{bmatrix}
              = \begin{bmatrix} | \\ A_2 \\ | \end{bmatrix}
              $$

              A general vector \(x\) with nonzero components
              in multiple coordinates gets mapped to a weighted sum of the columns of \(A\)

              $$
              y =  \begin{bmatrix}
                | & & | \\
                A_1 & \cdots & A_n \\
                | & & | \\
                \end{bmatrix}
                \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix}
              $$

              $$
              y = \begin{bmatrix} | \\ A_1 \\ | \end{bmatrix} x_1 + \cdots +
              \begin{bmatrix} | \\ A_n \\ | \end{bmatrix} x_n
              $$

              Another way of saying this is that \(Ax\) is a short hand for a linear
              combination of the columns of \(A\) with coefficients given by the elements of \(x\).

              Note that we can map sets of \(x\)'s (such as the simplex, unit sphere, etc) through
              \(A\) as well as points.  The span of the columns of \(A\) is called
              the "column space" or the "range" of \(A\) (more below).
            </p>

            </div>
          </div>


          <div class=col2>
            <div class='img1' style="padding:0px" id="multiply-container">
            </div>
            <div class="blurb1" style="padding:40px">

                  <b> MATRIX MULTIPLICATION </b>
                  <p>

                  When we multiply two matrices \(A\) and \(B\) together \(AB\)
                  we can think of \(A\) as acting on each of the columns of \(B\) separately

                  $$
                  AB =
                  \Bigg[\ \ A \ \ \Bigg]
                  \begin{bmatrix}
                  | &   & | \\
                  B_1  & \cdots & B_n \\
                  | &  & | \\
                  \end{bmatrix}
                  $$
                  $$
                  AB
                  =
                  \begin{bmatrix}
                  | &   & | \\
                  AB_1  & \cdots & AB_n \\
                  | &  & | \\
                  \end{bmatrix}
                  $$
                  One can use this to visualize the columns of many matrices
                  multiplied together
                  $$
                  M = ABCD...
                  $$.

                </p>

                </div>
              </div>


          <div class=col2>
            <div class='img1' style="padding:0px" id="coord-container">
            </div>
            <div class="blurb1" style="padding:40px">

                  <b> COORDINATES & BASES </b>
                  <p>

                  A vector \(x\) can be written as a linear combination of
                  the "standard basis" vectors with the coefficients being the
                  elements of the vector

                  $$ x =
                  \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}
                  \begin{bmatrix} 1 \\ 0 \\ \vdots \\ 0 \end{bmatrix} x_1
                  +
                  \begin{bmatrix} 0 \\ 1 \\ \vdots \\ 0 \end{bmatrix} x_2
                  + \cdots +
                  \begin{bmatrix} 0 \\ 0 \\ \vdots \\ 1 \end{bmatrix} x_n
                  $$
                  A "basis" for a space is a set of vectors that span the whole space
                  without any redundant vectors (more on this later).  Clearly any vector
                  can be written as a linear combination of the standard basis vectors.

                </p>

                <p>
                  A different set of vectors \(P_1,\dots,P_n\) could also be used as a basis for
                  \(\mathbb{R}^n\), meaning any vector \(x\) in the space can be written as a
                  linear combination of \(P_1,\dots,P_n\).  Writing these vectors as the columns of a matrix \(P\),
                  we can write this as
                  $$
                  x = Px'
                  $$

                  $$
                  \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix}
                  =
                  \begin{bmatrix}
                  | &  & | \\
                  P_1 & \cdots & P_n \\
                  | &  & |
                   \end{bmatrix}
                   \begin{bmatrix} x'_1 \\ \vdots \\ x'_n \end{bmatrix}
                  $$
                  The matrix \(P\) is called a "coordinate transformation" on the space \(\mathbb{R}^n \),
                  \(x\) is a "set of coordinates" with respect to the standard basis, and \(x'\) is a
                  set of coordinates with respect to the basis \(P\).

                  Any vector \(x\) can be written as a linear combination of the columns of \(P\)
                  assuming the columns of \(P\) form a basis.  These conditions are equivalent to saying
                  that \(P\) is "invertible".

                </p>

                </div>
              </div>





                  <div class=col2>
                    <div class='img1' style="padding:0px" id="inverse-container">
                    </div>
                    <div class="blurb1" style="padding:40px">

                          <b> INVERSES </b>
                          <p>

                          We say \(P\) is invertible if there is a unique \(x'\) that
                          solves \(x=Px'\) for any \(x \in \mathbb{R}^n \).  The phrase
                          "for any \(x\)" requires that the columns of \(P\) span \(\mathbb{R}^n \).
                          For \(x'\) to be unique, we need each column of \(P\) to point in a different
                          direction in \(\mathbb{R}^n\) (again more later).

                          The inverse of \(P\) is the matrix \(P^{-1}\) such that \(PP^{-1} = I \)
                          Expanding this equation we have that
                          $$
                          I = PP^{-1} =
                          \Bigg[
                          \ \ P \ \
                          \Bigg]
                          \begin{bmatrix}
                          | &   & | \\
                          Q_1  & \cdots & Q_n \\
                          | &  & |
                          \end{bmatrix}
                        =
                        \begin{bmatrix}
                        | &   & | \\
                        PQ_1  & \cdots & PQ_n \\
                        | &  & |
                        \end{bmatrix}
                        $$

                        Thus, \(Q_i\), the \(i\)th column of \(P^{-1}\) is the coordinates of
                        the \(i\)th standard basis vector with respect to the columns of P.
                        Setting \(x' = P^{-1}x\) then gives that \(x=Px'\) will be satisfied.
                        Note that for square, invertible \(P\), \(P^{-1}\) is unique and \(P^{-1}P=I\).
                        While maybe not surprising, proving these facts is slightly more involved.

                      </p>
                        </div>
                      </div>

                      <div class=col2>
                        <div class='img1' style="padding:0px" id="range-container">
                        </div>
                        <div class="blurb1" style="padding:40px">

                              <b> RANGE </b>
                              <p>

                              The "range" of a matrix \(A\) is the span of the columns,
                              ie. the set of vectors \(y\) in the co-domain for which
                              you could find an \(x\) such that \(y=Ax\).  If the range of
                              \(A\) is all of the co-domain, we say that the matrix \(A\) is onto.
                              As a rule of thumb, fat matrices are onto
                              (assuming there are enough linearly independent columns).
                              If a matrix is tall (or there are not enough linearly independent columns),
                              then there is a subspace of the co-domain that is not
                              reachable through \(A\).  An equation of the form \(y=Ax\) for a tall \(A\)
                              will usually not have a solution because it is not guaranteed that every \(y\)
                              is in the range of \(A\) (in other words, \(y=Ax\) is actually false for all \(x\)'s.)
                              At best, we can solve the equation \(\text{proj}_Ay = Ax\)
                              where \(\text{proj}_Ay\) is the closest vector to \(y\) within
                              the range of \(A\).  This is the well-studied "least-squares solution" in which
                              we choose \(x\) to minimize the norm-squared of \(y-Ax\)

                              $$
                              \min_x \quad \big|\big|y-Ax \big|\big|^2_2 = (y-Ax)^T(y-Ax)
                              $$
                              with the the optimal \(x\) given by

                              $$
                              x = (A^TA)^{-1}A^Ty
                              $$
                              \(Ax\) is then given by
                              $$
                              Ax = A(A^TA)^{-1}A^Ty
                              $$
                              (which is the projection of \(y\) onto the range of A).


                          </p>
                            </div>
                          </div>

                          <div class=col2>
                            <div class='img1' style="padding:0px" id="range-container">
                            </div>
                            <div class="blurb1" style="padding:40px">

                                  <b> NULLSPACE </b>
                                  <p>

                                  If some of the columns of \(A\) are linear
                                  combinations of other columns, then different sets of coordinates
                                  \(x\) could(will) produce the same resulting \(Ax\).
                                  For example, suppose for a \(3\times3\) matrix \(A\)
                                  the third column is a linear combination of the first two columns,
                                  ie.
                                  $$
                                  \begin{bmatrix} | \\ A_3 \\ |\end{bmatrix} =
                                  \begin{bmatrix} | \\ A_1 \\ |\end{bmatrix} w_1 +
                                  \begin{bmatrix} | \\ A_2 \\ |\end{bmatrix} w_2
                                  $$
                                  Note then that we get \(Ax=\mathbf{0}\) if we choose \(x\) in the following way
                                  $$
                                  Ax =
                                  \begin{bmatrix}
                                  | & | & | \\
                                  A_1 & A_2 & A_3 \\
                                  | & | & | \\
                                  \end{bmatrix}
                                  \begin{bmatrix}
                                  w_1 \\ w_2 \\ -1
                                  \end{bmatrix}
                                  =
                                  \begin{bmatrix} | \\ \mathbf{0} \\ | \end{bmatrix}
                                  $$
                                  The vector \(x\) is said to be in the "nullspace" of
                                  \(A\) since \(Ax=\mathbf{0}\). In other words,
                                  elements in the nullspace are coordinates of 0.
                                </p>

                                <p>
                                  In general for an \(m \times n\) matrix \(A\),
                                  suppose that all the columns of \(A\)
                                  lie in the span of the first \(k\) columns. we
                                  can extend the above technique to construct elements
                                  in the nullspace of \(A\) as follows.  Let \(B_j \in \mathbb{R}^k\)
                                  be the coordinates of column \(A_j\) with respect
                                  to the first \(k\) columns of \(A\), ie.

                                  $$
                                  \begin{bmatrix} | \\ A_j \\ |\end{bmatrix} =
                                  \begin{bmatrix}
                                  | &  & | \\
                                  A_1 & \cdots & A_k \\
                                  | &  & | \\
                                  \end{bmatrix}
                                  \begin{bmatrix} | \\ B_j \\ |\end{bmatrix}
                                  $$
                                  It follows then all the columns of the a matrix \(N \in \mathbb{R}^{n \times n-k}\)
                                  given below are in the nullspace of \(A\)
                                  $$
                                  AN =
                                  \bigg[\ \ A' \quad  A'' \bigg \ \ ]
                                  \begin{bmatrix}
                                  B \\ -I_{k+1}
                                  \end{bmatrix}
                                  = \bigg[ \ \ \mathbf{0} \ \ \bigg]
                                  $$
                                  where
                                  $$
                                  B =
                                  \small{
                                  \begin{bmatrix}
                                  | &  & | \\
                                  B_{k+1} & \cdots & B_n \\
                                  | &  & | 
                                  \end{bmatrix}
                                  }
                                  $$
                                  and
                                  $$
                                  \footnotesize{
                                  A' =
                                  \begin{bmatrix}
                                  | &  & | \\
                                  A_{1} & \cdots & A_k \\
                                  | &  & |
                                  \end{bmatrix}
                                  }
                                  \quad
                                  A'' =
                                  \begin{bmatrix}
                                  | &  & | \\
                                  A_{k+1} & \cdots & A_n \\
                                  | &  & |
                                  \end{bmatrix}
                                  }
                                  $$
                                  We can actually show that the columns of \(N\) constructed
                                  this way are actually a basis for the nullspace of \(A\).
                                  If we chose other columns (rather than the first \(k\)) to
                                  span the column space of \(A\), we could do the same construction
                                  but we would have to reorder the rows of \(N\).

                                </p>


                                <p>
                                  We say the nullspace of a matrix is "non-trivial"
                                  if it contains more than the zero-vector.
                                  (The zero-vector is obviously always in the nullspace.)
                                </p>
                                <p>
                                  If two sets of coordinates \(x\) and \(x'\) differ
                                  by an element of the nullspace, ie. \(x-x'\) is in the
                                  nullspace then they will both map through \(A\)
                                  to the same point
                                  $$
                                  0 = A(x-x') \qquad \Rightarrow \qquad Ax = Ax'
                                  $$
                                  This means equations of the form \(y=Ax\) do not have unique
                                  solutions when \(A\) has a non-trivial nullspace.


                              </p>
                                </div>
                              </div>




  <script type='module' src="./ndimo.js"> </script>
  <script type='module' src="./ndrawo.js"> </script>
  <script type='module' src="./hypershapeo.js"> </script>
  <script type='module' src="./vectoro.js"> </script>
  <script type='module' src="./matrixo.js"> </script>

  </body>
</html>
